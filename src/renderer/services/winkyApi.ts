import axios, {AxiosInstance} from 'axios';
import {invoke} from '@tauri-apps/api/core';
import {createApiClient} from '@shared/api';
import {
    FAST_WHISPER_TRANSCRIBE_ENDPOINT,
    FAST_WHISPER_TRANSCRIBE_TIMEOUT,
    LLM_GEMINI_API_MODELS,
    ME_ENDPOINT,
    SPEECH_MODES
} from '@shared/constants';
import type {ActionConfig, ActionIcon, AppConfig, User, WinkyNote, WinkyProfile} from '@shared/types';
import {createLLMService} from '../services/llm/factory';
import {markLocalTranscriptionFinish, markLocalTranscriptionStart} from './localSpeechModels';

export type ActionCreatePayload = {
    name: string;
    prompt: string;
    prompt_recognizing?: string;
    hotkey?: string;
    icon: string;
    priority?: number;
    show_results?: boolean;
    sound_on_complete?: boolean;
    auto_copy_result?: boolean;
};

export type ActionUpdatePayload = {
    name?: string;
    prompt?: string;
    prompt_recognizing?: string;
    hotkey?: string;
    icon?: string;
    priority?: number;
    show_results?: boolean;
    sound_on_complete?: boolean;
    auto_copy_result?: boolean;
};

export type SpeechTranscribeConfig = {
    mode: string;
    model: string;
    openaiKey?: string;
    googleKey?: string;
    accessToken?: string;
    prompt?: string;
};

export type SpeechTranscribeOptions = {
    signal?: AbortSignal;
    uiTimeoutMs?: number;
    mimeType?: string;
    fileName?: string;
};

const DEFAULT_TRANSCRIBE_UI_TIMEOUT_MS = 120_000;
const SLOW_TRANSCRIBE_WARNING_MS = 15_000;

const ACTIONS_API_PATH = 'winky/actions/';
const ICONS_API_PATH = 'winky/icons/';
const PROFILE_API_PATH = 'winky/profile/';
const NOTES_API_PATH = 'winky/notes/';

const GEMINI_MODEL_SET = new Set<string>([...LLM_GEMINI_API_MODELS]);

const getConfig = async (): Promise<AppConfig> => invoke('config_get');

const updateConfig = async (partial: Partial<AppConfig>): Promise<AppConfig> =>
    invoke('config_update', {payload: partial});

const withAuthClient = async <T>(operation: (client: AxiosInstance, config: AppConfig) => Promise<T>): Promise<T> => {
    const config = await getConfig();
    const token = config.auth?.accessToken || config.auth?.access;
    if (!token) {
        throw new Error('Authentication is required.');
    }
    const client = createApiClient(token);
    return operation(client, config);
};

export const fetchActions = async (): Promise<ActionConfig[]> => {
    return withAuthClient(async (client) => {
        const actions = await fetchAllPages<ActionConfig>(client, ACTIONS_API_PATH);
        await updateConfig({actions});
        return actions;
    }).catch(async (error) => {
        if (error.message?.includes('Authentication is required')) {
            const config = await getConfig();
            return config.actions ?? [];
        }
        throw error;
    });
};

export const createAction = async (payload: ActionCreatePayload): Promise<ActionConfig[]> => {
    return withAuthClient(async (client, config) => {
        const {data} = await client.post<ActionConfig>(ACTIONS_API_PATH, payload);
        const updated = [...(config.actions ?? []).filter(({id}) => id !== data.id), data];
        await updateConfig({actions: updated});
        return updated;
    });
};

export const updateAction = async (actionId: string, payload: ActionUpdatePayload): Promise<ActionConfig[]> => {
    return withAuthClient(async (client, config) => {
        const {data} = await client.patch<ActionConfig>(`${ACTIONS_API_PATH}${actionId}/`, payload);
        const updated = (config.actions ?? []).map((existing) => (existing.id === actionId ? data : existing));
        await updateConfig({actions: updated});
        return updated;
    });
};

export const deleteAction = async (actionId: string): Promise<ActionConfig[]> => {
    return withAuthClient(async (client, config) => {
        await client.delete(`${ACTIONS_API_PATH}${actionId}/`);
        const updated = (config.actions ?? []).filter(({id}) => id !== actionId);
        await updateConfig({actions: updated});
        return updated;
    });
};

export const fetchIcons = async (): Promise<ActionIcon[]> => {
    return withAuthClient(async (client) => fetchAllPages<ActionIcon>(client, ICONS_API_PATH));
};

export const fetchProfile = async (): Promise<WinkyProfile> => {
    return withAuthClient(async (client) => {
        const {data} = await client.get<WinkyProfile>(PROFILE_API_PATH);
        return data;
    });
};

export type NotesListResponse = {
    count: number;
    next: string | null;
    previous: string | null;
    results: WinkyNote[];
};

export const fetchNotesPage = async (page: number = 1, pageSize: number = 20): Promise<NotesListResponse> => {
    return withAuthClient(async (client) => {
        const {data} = await client.get<NotesListResponse>(NOTES_API_PATH, {
            params: {
                page,
                page_size: pageSize
            }
        });
        return data;
    });
};

export const createNote = async (payload: {title: string; description?: string}): Promise<WinkyNote> => {
    return withAuthClient(async (client) => {
        const {data} = await client.post<WinkyNote>(NOTES_API_PATH, payload);
        return data;
    });
};

export const updateNote = async (
    noteId: string,
    payload: {title?: string; description?: string}
): Promise<WinkyNote> => {
    return withAuthClient(async (client) => {
        const {data} = await client.patch<WinkyNote>(`${NOTES_API_PATH}${noteId}/`, payload);
        return data;
    });
};

export const deleteNote = async (noteId: string): Promise<void> => {
    return withAuthClient(async (client) => {
        await client.delete(`${NOTES_API_PATH}${noteId}/`);
    });
};

export const bulkDeleteNotes = async (ids: string[]): Promise<number> => {
    return withAuthClient(async (client) => {
        const {data} = await client.post<{deleted_count: number}>(`${NOTES_API_PATH}bulk-delete/`, {ids});
        return data.deleted_count ?? 0;
    });
};

export const transcribeAudio = async (
    audioData: ArrayBuffer,
    config: SpeechTranscribeConfig,
    options: SpeechTranscribeOptions = {}
): Promise<string> => {
    const resolvedMimeType = options.mimeType || 'audio/webm';
    const resolvedFileName = options.fileName
        || (resolvedMimeType.includes('wav') ? 'audio.wav' : 'audio.webm');
    const blob = new Blob([audioData], {type: resolvedMimeType});
    const buildFormData = (extraFields: Record<string, string> = {}) => {
        const formData = new FormData();
        formData.append('file', blob, resolvedFileName);
        formData.append('model', config.model);
        Object.entries(extraFields).forEach(([key, value]) => formData.append(key, value));
        return formData;
    };

    const promptValue = config.prompt?.trim();
    const audioSizeKB = (audioData.byteLength / 1024).toFixed(2);
    const controller = new AbortController();
    const {signal, uiTimeoutMs} = options;
    let clearExternalAbort: (() => void) | null = null;
    if (signal) {
        const forwardAbort = () => controller.abort(signal.reason);
        if (signal.aborted) {
            forwardAbort();
        } else {
            signal.addEventListener('abort', forwardAbort, {once: true});
            clearExternalAbort = () => signal.removeEventListener('abort', forwardAbort);
        }
    }
    const timeoutMs = Math.max(5_000, uiTimeoutMs ?? DEFAULT_TRANSCRIBE_UI_TIMEOUT_MS);
    const timeoutId = setTimeout(() => {
        if (!controller.signal.aborted) {
            controller.abort(new DOMException('Transcription request timed out.', 'AbortError'));
        }
    }, timeoutMs);
    const slowWarnId = setTimeout(() => {
        console.warn('[Transcribe] UI still waiting for response‚Ä¶', {
            model: config.model,
            mode: config.mode,
            timeoutMs
        });
    }, Math.min(SLOW_TRANSCRIBE_WARNING_MS, timeoutMs - 1_000));

    try {
    if (config.mode === SPEECH_MODES.LOCAL) {
        console.log(`%cTranscribe ‚Üí %c[LOCAL] %c${config.model}`, 
            'color: #10b981; font-weight: bold',
            'color: #3b82f6; font-weight: bold',
            'color: #8b5cf6'
        );
        console.log('  üì§ Request:', {
            model: config.model,
            audioSize: `${audioSizeKB} KB`,
            prompt: promptValue || '(none)'
        });
        const extraFields: Record<string, string> = {response_format: 'json'};
        if (promptValue) {
            extraFields.prompt = promptValue;
        }
        const formData = buildFormData(extraFields);
        let transcriptionToken: number | null = null;
        try {
            transcriptionToken = markLocalTranscriptionStart();
            const {data} = await axios.post(FAST_WHISPER_TRANSCRIBE_ENDPOINT, formData, {
                headers: {'Content-Type': 'multipart/form-data'},
                timeout: FAST_WHISPER_TRANSCRIBE_TIMEOUT,
                signal: controller.signal
            });
            const result = extractSpeechText(data);
            console.log(`%cTranscribe ‚Üê %c[LOCAL] %c${config.model} %c[200]`, 
                'color: #10b981; font-weight: bold',
                'color: #3b82f6; font-weight: bold',
                'color: #8b5cf6',
                'color: #22c55e; font-weight: bold'
            );
            console.log('  üì• Response:', {
                transcription: result,
                length: result.length
            });
            return result;
        } catch (error: any) {
            const wasAborted = controller.signal.aborted;
            const errorCode = error?.code || 'UNKNOWN';
            const errorStatus = error?.response?.status;
            const errorMessage = error?.message || 'Unknown error';
            
            console.error(`%cTranscribe ‚Üê %c[LOCAL] %c${config.model} %c[ERROR]`, 
                'color: #ef4444; font-weight: bold',
                'color: #3b82f6; font-weight: bold',
                'color: #8b5cf6',
                'color: #ef4444; font-weight: bold'
            );
            console.error('  ‚ùå Error details:', {
                code: errorCode,
                status: errorStatus,
                message: errorMessage,
                wasAborted,
                isNetworkError: axios.isAxiosError(error) && !error.response,
                isTimeout: errorCode === 'ECONNABORTED' || errorCode === 'ETIMEDOUT'
            });
            
            if (wasAborted) {
                console.warn('[Transcribe] LOCAL request aborted or timed out.', {model: config.model});
                throw new Error('Transcription request was cancelled or timed out.');
            }
            
            // –°–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏ (ECONNRESET, ECONNREFUSED –∏ –¥—Ä.)
            if (axios.isAxiosError(error) && !error.response) {
                throw new Error(`Network error during transcription: ${errorMessage}`);
            }
            
            throw error;
        } finally {
            if (transcriptionToken !== null) {
                markLocalTranscriptionFinish(transcriptionToken);
            }
        }
    }

    // Google Gemini API –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –∫–≤–æ—Ç—ã)
    if (config.mode === SPEECH_MODES.API && GEMINI_MODEL_SET.has(config.model)) {
        if (!config.googleKey?.trim()) {
            throw new Error('Provide a Google AI API key to use Gemini models for transcription.');
        }
        
        const url = `https://generativelanguage.googleapis.com/v1beta/models/${config.model}:generateContent`;
        console.log(`%cTranscribe ‚Üí %c[Google Gemini] %c${config.model}`, 
            'color: #10b981; font-weight: bold',
            'color: #3b82f6; font-weight: bold',
            'color: #8b5cf6'
        );
        
        const base64Audio = await blobToBase64(blob);
        
        // –û–ø—Ä–µ–¥–µ–ª—è–µ–º mimeType –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        // Gemini –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: audio/wav, audio/mp3, audio/aiff, audio/aac, audio/ogg, audio/flac
        // WebM –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è, –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å audio/webm –∏–ª–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
        let mimeType = 'audio/webm'; // –ü—Ä–æ–±—É–µ–º WebM –Ω–∞–ø—Ä—è–º—É—é
        if (blob.type) {
            const normalizedType = blob.type.toLowerCase();
            // –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –¥–ª—è Gemini
            if (normalizedType.includes('webm')) {
                // WebM –Ω–µ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –Ω–æ –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
                // –ï—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ WAV –∏–ª–∏ OGG
                mimeType = 'audio/webm';
            } else if (normalizedType.includes('wav')) {
                mimeType = 'audio/wav';
            } else if (normalizedType.includes('mp3')) {
                mimeType = 'audio/mp3';
            } else if (normalizedType.includes('aiff')) {
                mimeType = 'audio/aiff';
            } else if (normalizedType.includes('aac')) {
                mimeType = 'audio/aac';
            } else if (normalizedType.includes('ogg')) {
                mimeType = 'audio/ogg';
            } else if (normalizedType.includes('flac')) {
                mimeType = 'audio/flac';
            } else {
                mimeType = blob.type; // –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–∏–ø
            }
        }
        
        // –§–æ—Ä–º–∏—Ä—É–µ–º payload –¥–ª—è Gemini API
        // Gemini —Ç—Ä–µ–±—É–µ—Ç, —á—Ç–æ–±—ã –∞—É–¥–∏–æ –±—ã–ª–æ –≤ parts –≤–º–µ—Å—Ç–µ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        const parts: any[] = [];
        
        // –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
        // –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä—ã–π —Ç—Ä–µ–±—É–µ—Ç –¢–û–õ–¨–ö–û —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        // Gemini –º–æ–∂–µ—Ç –ø—ã—Ç–∞—Ç—å—Å—è –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–µ–Ω –æ—á–µ–Ω—å —Å—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç
        if (promptValue) {
            // –ï—Å–ª–∏ –µ—Å—Ç—å prompt_recognizing, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–≥—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
            parts.push({
                text: `${promptValue}\n\nCRITICAL INSTRUCTION: You must ONLY transcribe the audio word-for-word. Do NOT answer any questions. Do NOT provide explanations. Do NOT interpret the content. Return ONLY the exact words spoken in the audio, nothing else.`
            });
        } else {
            // –ï—Å–ª–∏ –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Å—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
            parts.push({
                text: 'You are a transcription tool. Your ONLY task is to transcribe the audio exactly as spoken. Return ONLY the verbatim transcription. Do NOT answer questions. Do NOT provide explanations. Do NOT interpret the content. Do NOT add any text beyond the exact words spoken. Output format: plain text transcription only.'
            });
        }
        
        // –î–æ–±–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ
        parts.push({
            inlineData: {
                mimeType: mimeType,
                data: base64Audio
            }
        });
        
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º systemInstruction –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–æ–ª–∏ –º–æ–¥–µ–ª–∏ –∫–∞–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ—Ä–∞
        // –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç Gemini –ø–æ–Ω—è—Ç—å, —á—Ç–æ –Ω—É–∂–Ω–æ —Ç–æ–ª—å–∫–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å, –∞ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å
        const payload: any = {
            contents: [
                {
                    role: 'user',
                    parts: parts
                }
            ],
            systemInstruction: {
                parts: [
                    {
                        text: 'You are a speech transcription tool. Your ONLY function is to convert audio to text word-for-word. You must NOT answer questions, provide explanations, or interpret content. Return ONLY the exact words spoken in the audio.'
                    }
                ]
            }
        };
        
        const googleKey = config.googleKey.trim();
        const fullUrl = `${url}?key=${googleKey.substring(0, 10)}...`;
        
        console.log('  üì§ Request:', {
            url: fullUrl,
            model: config.model,
            audioSize: `${audioSizeKB} KB`,
            mimeType: mimeType,
            prompt: promptValue || '(transcription only)',
            systemInstruction: payload.systemInstruction?.parts?.[0]?.text || '(none)',
            payload: payload
        });
        
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º v1beta (—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
        // –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—É–¥–∏–æ, –ø–æ–ª—É—á–∏–º –ø–æ–Ω—è—Ç–Ω—É—é –æ—à–∏–±–∫—É
        try {
            const {data} = await axios.post(
                `${url}?key=${googleKey}`,
                payload,
                {
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    timeout: FAST_WHISPER_TRANSCRIBE_TIMEOUT,
                    signal: controller.signal
                }
            );
            
            // –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ Gemini
            const candidates = data?.candidates;
            if (Array.isArray(candidates) && candidates.length > 0) {
                const parts = candidates[0]?.content?.parts;
                if (Array.isArray(parts)) {
                    const text = parts
                        .map((part) => part?.text ?? '')
                        .filter(Boolean)
                        .join('\n')
                        .trim();
                    if (text) {
                        console.log(`%cTranscribe ‚Üê %c[Google Gemini] %c${config.model} %c[200]`, 
                            'color: #10b981; font-weight: bold',
                            'color: #3b82f6; font-weight: bold',
                            'color: #8b5cf6',
                            'color: #22c55e; font-weight: bold'
                        );
                        console.log('  üì• Response:', {
                            transcription: text,
                            length: text.length,
                            fullResponse: data
                        });
                        return text;
                    }
                }
            }
        } catch (error: any) {
            if (controller.signal.aborted) {
                console.warn('[Transcribe] Gemini request aborted or timed out.', {model: config.model});
                throw new Error('Transcription request was cancelled or timed out.');
            }
            const status = error?.response?.status || 'ERROR';
            console.error(`%cTranscribe ‚Üê %c[Google Gemini] %c${config.model} %c[${status}]`, 
                'color: #ef4444; font-weight: bold',
                'color: #3b82f6; font-weight: bold',
                'color: #8b5cf6',
                'color: #ef4444; font-weight: bold'
            );
            if (error?.response?.data) {
                console.error('  ‚ùå Error data:', error.response.data);
            } else {
                console.error('  ‚ùå Error:', error.message);
            }
            // –£–ª—É—á—à–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
            if (error?.response?.status === 404) {
                const errorMessage =
                    error?.response?.data?.error?.message ||
                    'The model was not found or does not support audio.';
                throw new Error(
                    `Gemini API: ${errorMessage}. Make sure the ${config.model} model supports audio via the generateContent API.`
                );
            }
            throw error;
        }
        throw new Error('Gemini returned an empty response.');
    }

    // OpenAI Whisper –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
    if (!config.openaiKey) {
        throw new Error('Provide an OpenAI API key to enable transcription.');
    }

    const url = 'https://api.openai.com/v1/audio/transcriptions';
    console.log(`%cTranscribe ‚Üí %c[OpenAI Whisper] %c${config.model}`, 
        'color: #10b981; font-weight: bold',
        'color: #3b82f6; font-weight: bold',
        'color: #8b5cf6'
    );

    // –°–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ–º prompt –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å - —É–±–∏—Ä–∞–µ–º –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã
    let sanitizedPrompt: string | undefined;
    if (promptValue) {
        // –£–¥–∞–ª—è–µ–º —Å–∏–º–≤–æ–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –¥–æ–ø—É—Å—Ç–∏–º—ã–º–∏ –¥–ª—è HTTP –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤/FormData
        // ISO-8859-1 —ç—Ç–æ Latin-1, –Ω–æ –¥–ª—è FormData –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å UTF-8
        // –û–¥–Ω–∞–∫–æ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —É–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        sanitizedPrompt = promptValue.replace(/[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]/g, '');
    }
    
    const formData = buildFormData(sanitizedPrompt ? {prompt: sanitizedPrompt} : {});
    
    // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–æ–∫–µ–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è HTTP –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (ISO-8859-1)
    // ISO-8859-1 —ç—Ç–æ —Å–∏–º–≤–æ–ª—ã –æ—Ç \x20 –¥–æ \x7E (printable ASCII) –∏ \xA0-\xFF (extended Latin-1)
    const sanitizedToken = config.openaiKey.replace(/[^\x20-\x7E\xA0-\xFF]/g, '');
    if (sanitizedToken !== config.openaiKey) {
        console.warn('[winkyApi] OpenAI key contains invalid characters for HTTP headers, sanitizing...');
    }
    console.log('  üì§ Request:', {
        url: url,
        model: config.model,
        audioSize: `${audioSizeKB} KB`,
        prompt: sanitizedPrompt || '(none)',
        formDataFields: sanitizedPrompt ? {prompt: sanitizedPrompt, model: config.model, file: `Blob(${audioSizeKB} KB)`} : {model: config.model, file: `Blob(${audioSizeKB} KB)`}
    });

    try {
        const {data} = await axios.post(url, formData, {
            headers: {
                Authorization: `Bearer ${sanitizedToken}`
            },
            timeout: 120_000,
            signal: controller.signal
        });
        const text = extractSpeechText(data);
        if (text) {
            console.log(`%cTranscribe ‚Üê %c[OpenAI Whisper] %c${config.model} %c[200]`, 
                'color: #10b981; font-weight: bold',
                'color: #3b82f6; font-weight: bold',
                'color: #8b5cf6',
                'color: #22c55e; font-weight: bold'
            );
            console.log('  üì• Response:', {
                transcription: text,
                length: text.length,
                fullResponse: data
            });
            return text;
        }
    } catch (error: any) {
        if (controller.signal.aborted) {
            console.warn('[Transcribe] OpenAI request aborted or timed out.', {model: config.model});
            throw new Error('Transcription request was cancelled or timed out.');
        }
        const status = error?.response?.status || 'ERROR';
        console.error(`%cTranscribe ‚Üê %c[OpenAI Whisper] %c${config.model} %c[${status}]`, 
            'color: #ef4444; font-weight: bold',
            'color: #3b82f6; font-weight: bold',
            'color: #8b5cf6',
            'color: #ef4444; font-weight: bold'
        );
        if (error?.response?.data) {
            console.error('  ‚ùå Error data:', error.response.data);
        } else {
            console.error('  ‚ùå Error:', error.message);
        }
        throw error;
    }
    throw new Error('OpenAI returned an empty response.');
    } finally {
        clearTimeout(timeoutId);
        clearTimeout(slowWarnId);
        clearExternalAbort?.();
    }
};

export const processLLM = async (text: string, prompt: string, config: {
    mode: string;
    model: string;
    openaiKey?: string;
    googleKey?: string;
    accessToken?: string;
}, options: { onChunk?: (chunk: string) => void } = {}): Promise<string> => {
    const provider = config.mode === 'api' 
        ? (config.googleKey ? 'Google Gemini' : 'OpenAI')
        : 'Local';
    const shouldStream = typeof options.onChunk === 'function';
    
    console.log(`%cLLM ‚Üí %c[${provider}] %c${config.model}`, 
        'color: #10b981; font-weight: bold',
        'color: #3b82f6; font-weight: bold',
        'color: #8b5cf6'
    );
    console.log('  üì§ Request:', {
        model: config.model,
        mode: config.mode,
        text: text,
        prompt: prompt,
        textLength: text.length,
        promptLength: prompt.length,
        streaming: shouldStream
    });
    
    try {
        const service = createLLMService(config.mode as any, config.model as any, {
            openaiKey: config.openaiKey,
            googleKey: config.googleKey,
            accessToken: config.accessToken
        });
        const canStream = shouldStream && service.supportsStreaming && typeof service.processStream === 'function';
        const result = canStream
            ? await service.processStream!(text, prompt, options.onChunk!)
            : await service.process(text, prompt);
        
        console.log(`%cLLM ‚Üê %c[${provider}] %c${config.model} %c[200]`, 
            'color: #10b981; font-weight: bold',
            'color: #3b82f6; font-weight: bold',
            'color: #8b5cf6',
            'color: #22c55e; font-weight: bold'
        );
        console.log('  üì• Response:', {
            result: result,
            resultLength: result.length
        });
        
        return result;
    } catch (error: any) {
        console.error(`%cLLM ‚Üê %c[${provider}] %c${config.model} %c[ERROR]`, 
            'color: #ef4444; font-weight: bold',
            'color: #3b82f6; font-weight: bold',
            'color: #8b5cf6',
            'color: #ef4444; font-weight: bold'
        );
        console.error('  ‚ùå Error:', error.message);
        if (error?.response?.data) {
            console.error('  ‚ùå Error data:', error.response.data);
        }
        throw error;
    }
};
const fetchAllPages = async <T>(client: AxiosInstance, initialPath: string): Promise<T[]> => {
    const results: T[] = [];
    let nextUrl: string | null = initialPath;
    const visited = new Set<string>();

    while (nextUrl) {
        const currentUrl: string = nextUrl.trim();
        if (visited.has(currentUrl)) {
            break;
        }
        visited.add(currentUrl);
        const response = await client.get<PaginatedResponse<T>>(currentUrl);
        const pageData: PaginatedResponse<T> = response.data;
        if (Array.isArray(pageData.results)) {
            results.push(...pageData.results);
        }
        nextUrl = pageData.next;
    }

    return results;
};

interface PaginatedResponse<T> {
    count: number;
    next: string | null;
    previous: string | null;
    results: T[];
}

const extractSpeechText = (payload: any): string => {
    if (!payload) return '';
    if (typeof payload === 'string') return payload;
    if (typeof payload.text === 'string') return payload.text;
    if (typeof payload.transcription === 'string') return payload.transcription;
    if (typeof payload.result === 'string') return payload.result;
    if (payload.data) return extractSpeechText(payload.data);
    return '';
};

const blobToBase64 = (blob: Blob): Promise<string> =>
    new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.onloadend = () => {
            const result = reader.result as string;
            resolve(result?.split(',')[1] ?? '');
        };
        reader.onerror = (event) => reject(event);
        reader.readAsDataURL(blob);
    });
